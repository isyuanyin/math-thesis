\section{背景知识}
\frame
{
	\frametitle{\secname~ }
	\begin{block}{基本概念}
		马尔可夫决策过程、回报、价值函数
	\end{block}
	\begin{block}{实践中的强化学习}
		经典的Q学习、REINFORCE和A2C算法
	\end{block}
}

\subsection*{基本概念}
\frame{
	% \frametitle{马尔可夫决策过程（MDP）}
	\begin{block}{马尔可夫决策过程（MDP）}
		智能体与环境交互的过程产生的序列如下：
		\begin{equation}
			S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3,\dots
			\label{eq:mdp-sequence}
		\end{equation}
		且这样的序列满足如下条件：
		\begin{equation}
			\mathbb{P}[S_t = s', R_t = r| S_{t-1}, A_{t-1} = a] = p(s', r | s, a)
			\label{eq:markov-condition-probability}
		\end{equation}
		可知序列中的状态 $S_t$ 满足马尔可夫性质，而该序列是一个\textbf{马尔可夫决策过程}。
	\end{block}
}

\frame{
	% \frametitle{回报与折扣、策略}
	% \vspace{-0.8em}
	\begin{block}{回报}
		带有折扣$\gamma$的回报：
		\begin{equation}
			G_t = \sum_{k \geq 1} \gamma^{k-1} R_{t+k} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots
			\label{eq:return}
		\end{equation}
	\end{block}

	\begin{block}{策略}
		策略$\pi$表示智能体在某个状态$s$选择动作$a$的概率，
		\begin{equation}
				\begin{split}
					\pi(a|s) = \mathbb{P} [A_t = a | S_t = s]
				\end{split}
		\end{equation}
	\end{block}
}

\frame{
	% \frametitle{价值函数}
	\begin{block}{价值函数}
		% \small
		\begin{itemize}
			\item  状态价值函数：
			\begin{equation}
				v_\pi (s) = \mathbb{E}_\pi [G_t | S_t = s]
				\label{eq:state-value-function}
			\end{equation}

			\item 状态价值函数方程：
			\begin{equation}
				v_\pi (s) = \sum_a \pi(a|s) \left[ \sum_{s', r} p(s', r | s, a)  [r + \gamma v_\pi(s')] \right]
				\label{eq:constraint}
			\end{equation}
		\end{itemize}
	\end{block}
}

\subsection*{实践中的强化学习}
\frame{
	% \frametitle{Q学习和REINFORCE算法}
	\footnotesize
	\begin{block}{Q学习}
		\begin{itemize}
			\item 贝尔曼方程：
			\begin{equation}
				v_*(s) = \max_a \sum_{s', r} p(s', r | s, a) \left[r + \gamma v_*(s') \right]
				\label{eq:bellman-equation}
			\end{equation}

			\item 利用蒙特卡洛采样和贝尔曼方程的迭代求解得到最优的价值函数，进而确定策略$\pi$
		\end{itemize}
		
	\end{block}
	\begin{block}{REINFORCE}
		将策略参数化为$\pi_\theta$，直接利用蒙特卡洛采样的结果调整$\pi_\theta$，最大化
		\begin{equation}
			J(\theta) = \mathbb{E}_\pi \left[ G_t \cdot  \ln(\pi(A_t|S_t, \theta))  \right]
		\end{equation}
	\end{block}
}

\frame{
	% \frametitle{A2C算法}
	\begin{block}{A2C}
		用迭代法估计价值函数，利用价值函数和蒙特卡洛采样估计回报 $G_t$，然后用REINFORCE算法更新策略。
	\end{block}

	\begin{block}{问题}
		\begin{itemize}
			\item 大部分的深度强化学习方法使用神经网络逼近上述的价值函数和策略
			\item 无法完美解决一些挑战，例如奖励延迟、不完全观测和奖励干扰等难题
		\end{itemize}
	\end{block}

}


\endinput
