%% chapter 4 dataset, network structure, experiment and result
\chapter{总结}
\label{cha:experiment}

本文尝试通过结合发现“预测内容”和“如何进行引导”来元学习完整的RL更新规则，从而取代了现有的RL概念（例如价值函数和TD学习）。一小组非常简单的环境的实验结果表明，发现的LPG可以在预测中保留丰富的信息，这对于有效的引导非常重要。

我们认为，这仅仅是完全数据驱动的RL算法发现的开始；
从我们的程序生成环境到新的高级体系结构以及替代的产生经验的方法，有许多很有希望的方向来扩展我们的工作。
从玩具领域到Atari游戏的激进概括表明，从与环境的交互中发现有效的RL算法可能是可行的，这有可能导致全新的RL方法。
广泛的影响所提出的方法有可能通过以数据驱动的方式使发现过程自动化来极大地加快发现新的强化学习（RL）算法的过程。
如果所提出的研究方向成功，这可能会将研究范式从人工开发RL算法转变为构建适当的环境集，从而使所得算法高效。
此外，建议的方法还可以用作协助RL研究人员开发和改进其手工设计算法的工具。
在这种情况下，所提出的方法可用于根据研究人员作为输入提供的体系结构，提供有关良好更新规则的外观的见解，从而可以加快RL算法的手动发现。
另一方面，由于所提出方法的数据驱动性质，所得到的算法可能会捕获环境训练集中的意外偏差。在我们的工作中，除了发现算法时的奖励，我们没有提供特定领域的信息，这使得算法很难在训练环境中捕获偏置。但是，需要更多的工作来消除发现的算法中的偏差，以防止潜在的负面结果。