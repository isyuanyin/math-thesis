\chapter{实验}
\label{cha:method}



实验旨在回答以下研究问题：
\begin{itemize}
    \item LPG能否发现预测的有用语义，以进行有效的引导？
    \item 发现的预测语义是什么？ 
    \item 发现预测的语义有多重要？ 
    \item 调节器和超参数的平衡有多重要？ 
    \item LPG可以从玩具环境推广到复杂的Atari游戏吗？ 
\end{itemize}

\section{实验配置}
训练环境为了进行LPG的元训练，我们引入了三种不同的玩具域，如图2所示。表格网格世界是具有固定对象位置的网格世界。

随机网格世界的每个情节都有随机的对象位置。

延迟链MDP是具有延迟奖励的简单MDP。

每个域有5种不同的环境，具有不同数量的奖励状态和情节长度。

培训环境旨在捕获基本的RL挑战，例如延迟奖励，嘈杂奖励和长期信用分配。

大多数训练环境都是表格形式的，没有涉及任何函数逼近器。

补充材料中描述了所有环境的详细信息。

实现细节我们使用了30维预测向量y∈[0，1] 30。

在元训练期间，我们每20个时间步更新一次代理参数。

由于大多数训练情节跨越20-2000步，因此LPG必须为预测y发现长期语义，以便能够最大化来自部分轨迹的长期未来回报。

该算法是使用JAX [5]实现的。

补充材料中描述了更多的实现细节。

基线如第2节和表1所述，除MetaGenRL [17]外，大多数先前的工作不支持在完全不同的环境中进行概括。

但是，MetaGenRL设计用于连续控制并且基于DDPG [29，18]。

相反，为了研究发现预测语义的重要性，我们将其与自己的基准LPG-V（LPG的变体）进行比较，该变体与MetaGenRL一样，仅在给定由TD（λ）训练的值函数的情况下才学习策略更新规则（ˆπ）[ 

[31]没有发现自己的预测语义。1此外，我们还将优势参与者评论家（A2C）[23]作为人类发现的标准算法基线进行了比较。 

4.2专门用于训练环境我们评估了训练环境上的LPG，以查看LPG是否已发现有效的更新规则。

图3中的结果表明，在大多数训练中，LPG均胜过A2C