\chapter{相关工作与研究现状}
\label{cha:sysu-thesis-latex-install-guide}

本章


\section{元学习}


关于学习学习的早期工作已经以各种方式讨论了学习学习的思想，例如改进遗传程序设计[27]，学习神经网络更新规则[4]，学习率适应[30]，自重

修改RNN [28]和领域不​​变知识的转移[32]。

此类工作表明，不仅可以学习优化固定目标，而且可以改进在元级别进行优化的方式。

为少量任务适应而学习学习在少拍学习的背景下，学习受到了很多关注[26，34]。 

MAML [10，11]允许通过反向传播参数更新来元学习初始参数。 

RL2 [8，35]通过在代理的整个生命周期中展开LSTM [15]，将学习本身描述为RL问题。

其他方法包括简单逼近[25]，具有Hebbian学习的RNN [21、22]和梯度预处理[12]。

所有这些在代理程序和算法之间都没有明确区分，因此，根据问题的定义，生成的元学习算法特定于单个代理程序体系结构。

为单项任务在线适应学习学习不同语料库的工作重点是学习在单个生命周期内学习单个任务。

徐等。 

[38]引入了元梯度RL方法；

它使用反向传播遍历代理程序的更新，以计算相对于更新的元参数的梯度。

该方法已应用于元学习各种形式的算法组件，例如折扣因子[38]，内在奖励[41]，辅助任务[33]，回报[36]，辅助策略更新[42]，非政策

更正[39]，并更新目标[37]。

相反，我们的工作有一个正交的目标：发现对更广泛的代理和环境类别有效的通用算法，而不是适应特定的环境。

发现强化学习算法从早期关于强盗算法[20，19]到好奇算法[1]和RL目标[16，40，6，17]的尝试，元学习RL算法的尝试（见表1）

进行比较）。 

EPG [16]使用进化策略来找到策略更新规则。

郑等。 

[40]表明，勘探的一般知识可以通过奖励函数的形式获得。 

ML3 [6]使用元梯度对损失函数进行元学习。

但是，先前的工作最多只能将相同领域中的相似任务概括化。

最近，提出了MetaGenRL [17]来元学习领域不变的策略更新规则，该规则能够从几个MuJoCo环境推广到其他MuJoCo环境。

然而很少有先前的工作试图发现完整的更新规则；

取而代之的是，它们全都依赖于价值函数（可以说是RL的最基本构建块）进行引导。

相反，我们的LPG元学习自举机制。

此外，本文是第一个表明从玩具环境到具有挑战性的基准的根本性概括的论文。

\section{编译环境配置}


\subsection{编译环境配置：Window篇}


\subsection{编译环境配置：Linux篇}




% 首先将个人信息写到\texttt{./docs/info.tex}中。