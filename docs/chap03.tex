\chapter{相关工作与研究现状}
\label{cha:sysu-thesis-latex-install-guide}

本章介绍与本文类似的相关工作


% 介绍早期的元学习的研究思想
\section{比较早期的工作}
学习如何去学习（learning to learn），有时候也称为元学习（meta learning）\cite{MetaLearningComputer2021} 在早期工作里已经有各式各样的讨论。例如改进遗传程序设计\cite{ohDiscoveringReinforcementLearning2020} \cite{schmidhuberEvolutionaryPrinciplesSelfreferential1987}，学习神经网络更新规则 \cite{bengioLearningSynapticLearning1990}，学习率适应 \cite{suttonAdaptingBiasGradient1992} 和领域不变知识的转移\cite{thrunLearningOneMore1994}等等。此类工作表明，不仅可以学习优化固定目标，而且可以改进在元级别进行优化的方式，从而解决人为设计学习规则不是完全适应问题本身的难题。


% 介绍普适性的元学习方法
\section{针对小样本的元学习}
对于小样本情形的机器学习，需要更高层次地设计，早期的一些工作从各个方面进行了研究。
Vinyals等人\cite{vinyalsMatchingNetworksOne2016b}（2016年）设计了可以学习一个网络结构的框架，有效地提升了图像分类任务的少样本学习的准确率。
Santoro 等人\cite{santoroMetaLearningMemoryAugmentedNeural2016}（2016年）设计了可以将过去的学习信息存储的增强记忆神经网络，该网络可以快速吸收新的数据，并利用此数据仅需几个样本就可以做出准确的预测。
Finn等人提出的 MAML（2017年）\cite{finnModelagnosticMetalearningFast2017} 和后来提出更通用的版本 \cite{finnMetaLearningUniversalityDeep2018}（2018年），通过先验的优化求解过程寻找更加适合求解优化问题的初始参数。
2017年 Duan 等人提出的 RL$^2$ 算法 \cite{duanRLFastReinforcement2016} 针对学习过程中实验次数过多问题，通过在代理的整个生命周期中展开LSTM，将学习本身描述为RL问题，从而利用过去的学习经验，减少在当前环境的训练次数。
所有这些方法都没有明确区分智能体和算法，所以，这些元学习算法也适用于单个智能体的程序结构。


\section{针对单一任务在线的元学习}
Xu等人\cite{xuMetagradientReinforcementLearning2018} 引入了元梯度RL方法；它使用反向传播遍历代理程序的更新，以计算相对于更新的元参数的梯度。
相反，本文的工作有一个正交的目标：发现对更广泛的代理和环境类别有效的通用算法，而不是适应特定的环境。

\section{自学习的强化学习算法}
EPG [16]使用进化策略来找到策略更新规则。
郑等人[40]表明，勘探的一般知识可以通过奖励函数的形式获得。 
ML3 [6]使用元梯度对损失函数进行元学习。
但是，先前的工作最多只能将相同领域中的相似任务概括化。
最近，提出了MetaGenRL \cite{kirschImprovingGeneralizationMeta2019} 来元学习领域不变的策略更新规则，该规则能够从几个MuJoCo环境推广到其他MuJoCo环境。
然而很少有先前的工作试图发现完整的更新规则；取而代之的是，它们全都依赖于价值函数（可以说是RL的最基本构建块）进行引导。
DeepMind公司在2020年提出的LPG元学习自举机制\cite{ohDiscoveringReinforcementLearning2020}。