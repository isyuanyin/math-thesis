%%
% 背景知识
%%

\chapter{背景知识}
\label{cha:background-knowledge}

强化学习是为了处理现实中的问题而提出的概念。Sutton 在自己的著作 \cite{suttonReinforcementLearningIntroduction2018}中，将强化学习问题抽象成马尔可夫决策过程（Markov Decision Program, MDP），这也是目前机器学习领域的共识 \cite{mnihHumanlevelControlDeep2015} \cite{mnihAsynchronousMethodsDeep2016} \cite{hesselRainbowCombiningImprovements2018}  ，本文也以此为基础介绍相关的概念和理论。这一章将从马尔可夫决策过程出发，解释强化学习的基本概念，以及介绍一些实践中遇到的问题及其经典的解决方法。

\section{基本概念}
\subsection{马尔可夫决策过程}

如第一章所言，我们假设智能体和环境的交互是离散的，如图 \ref{fig:agent-env-interaction} 所示， $t$ 的取值范围是 $\{0, 1, 2,3, \dots\}$。在每一个 $t$ 时刻依次发生如下事件：
\begin{itemize}
    \item 智能体观察处于状态 $S_t \in \mathcal{S}$ 的环境，得到观测 $O_t \in \mathcal{O}$，其中 $\mathcal{S}$ 是假设的状态空间，包含所有可能的状态；$\mathcal{O}$ 是假设的观测空间，包含所有可能的观测值。
    \item 智能体根据观测决定做出动作 $A_t \in \mathcal{A}$，其中 $\mathcal{A}$ 是动作空间。
    \item 在智能体做出动作后，环境会反馈一个奖励 $R_{t+1} \in \mathcal{R}$ 给智能体，其中$\mathcal{R}$ 是所有可能奖励的集合，一般假设 $\mathcal{R} \subset \mathbb{R}$。
\end{itemize}

为了叙述简单，假设智能体总是可以观测到环境的状态，即 $S_t = O_t$。智能体与环境交互的过程产生的序列如下：
\begin{equation}
    S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3,\dots
    \label{eq:mdp-sequence}
\end{equation}
在此基础上，进一步假设 奖励 $R_{t+1}$ 和下一个状态 $S_{t+1}$ 仅仅依赖当前的状态 $S_t$ 和动作 $A_t$，这称之为马尔可夫性质。用概率论的观点描述这种依赖就是：
\begin{equation}
    \mathbb{P}[S_t = s', R_t = r| S_{t-1}, A_{t-1} = a] = p(s', r | s, a)
    \label{eq:markov-condition-probability}
\end{equation}
恒满足等式 \ref{eq:markov-condition-probability} 的序列 \ref{eq:mdp-sequence} 就是马尔可夫决策过程的模型，表示 $p(s', r | s, a)$ 决定了所有时间上的状态转移和所得奖励的概率。在马尔可夫决策过程中，智能体选择一个策略（policy），在交互的每个时刻根据状态（观测）做出动作，用函数 $\pi : \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$ 表示策略，即选择某个动作的概率，
\begin{equation}
        \begin{split}
            \pi(a|s) = \mathbb{P} [A_t = a | S_t = s]
        \end{split}
\end{equation}

% 上述的模型可以导出一些更加直观的结论，
% \begin{itemize}
%     \item 状态转移概率：
%     \begin{equation}
%         p(s'|s, a) = \mathcal{P}[S_{t+1} = s' | S_{t} = s, A_{t} = a] = \sum_{r\in \mathcal{R}} p(s', r|s, a)
%     \end{equation}
%     \item “状态-动作”的期望奖励：
%     \begin{equation}
%         r(s, a) = \mathbb{E} [R_{t+1} | S_{t} = s, A_{t} = a] = \sum_{r\in \mathcal{R}} r \sum_{s' \in \mathcal{S}} p(s', r | s, a)
%     \end{equation}
%     \item “状态-动作-状态”的期望奖励：
%     \begin{equation}
%         r(s, a, s') = \mathbb{E} [R_{t+1} | S_{t} = s, A_{t-1} = a] = \sum_{r\in \mathcal{R}} r p(s', r | s, a)
%     \end{equation}
% \end{itemize}


\subsection{回报、折扣和值函数}
强化学习的目标是最大化累积的奖励，用回报（return）表示累积的奖励，则从 $t$ 时刻之后的回报 $G_t$ 定义为：
\begin{equation}
    G_t = \sum_{k \geq 1} R_{t+k} = R_{t+1} + R_{t+2} + R_{t+3} + \dots
\end{equation}

通过累积计算出来的回报往往是无穷大，通常会引入折扣（discount），记为$\gamma \in [0,1]$ 重新定义回报：
\begin{equation}
    G_t = \sum_{k \geq 1} \gamma^{k-1} R_{t+k} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots
    \label{eq:return}
\end{equation}

对于给定的策略 $\pi$，可以计算状态 $S_t = s$ 的期望回报，记为
\begin{equation}
    v_\pi (s) = \mathbb{E}_\pi [G_t | S_t = s]
    \label{eq:state-value-function}
\end{equation}
当在状态 $s$ 采取一个确定的动作 $a$ 时，期望回报重新计算为如下，
\begin{equation}
    q_\pi(s, a) = \mathbb{E} [G_t | S_t = s, A_t = a]
    \label{eq:action-value-function}
\end{equation}
这两个函数分别称为状态价值函数和状态-动作价值函数。显然，两者的关系为 $ v_\pi(s) = \sum_{a} \pi(a|s) q_{\pi} (s, a)$。

\subsection{最优策略}
假设初始状态 $S_0 = s$ 的概率为 $p_0(s)$，则强化学习的求解目标自然是，
\begin{equation}
    \max_\pi \sum_s v_\pi(s) p_0(s)
    \label{eq:subject}
\end{equation}
而公式 \ref{eq:state-value-function} 的计算可以简化为各个状态 $v_\pi(s)$ 之间的计算关系，即关于$v_\pi(s)$的方程
\begin{equation}
    v_\pi (s) = \sum_a \pi(a|s) \left[ \sum_{s', r} p(s', r | s, a)  [r + \gamma v_\pi(s')] \right]
    \label{eq:constraint}
\end{equation}
目标 \ref{eq:subject} 和 约束 \ref{eq:constraint} 以及策略函数 $\pi(a|s)$ 自身的约束是一个线性规划问题。同样地，我们可以利用线性规划问题的求解方法求解该问题。

但是，进一步地，可以证明\cite{suttonReinforcementLearningIntroduction2018}总是存在一个策略 $\pi_*$，使得 $v_{\pi_*}(s) \geq v_\pi(s), \; \forall s,\pi$，这样的策略称为最优策略。记 $v_* = v_{\pi_*}$ 和 $q_* = q_{\pi_*}$，很显然，这样的策略满足：
\begin{equation}
    v_*(s) = \max_{a} q_*(s, a) = \max_a \sum_{s', r} p(s', r | s, a) \left[r + \gamma v_*(s') \right]
    \label{eq:bellman-equation}
\end{equation}
公式 \ref{eq:bellman-equation} 称为贝尔曼方程，将之替换公式 \ref{eq:constraint}，可以求解出 $v_*$，同时确定了最优策略 $\pi_*$。

\section{实践中的强化学习}
\subsection{价值迭代计算}
在求解方程 \ref{eq:constraint} 时，可以利用雅可比迭代法或者高斯-赛德尔迭代法，即通过下面迭代公式解线性方程组。
\begin{equation}
    v_{k+1} = \sum_a \pi(a|s) \left[ \sum_{s', r} p(s', r | s, a)  \left[r + \gamma v_k(s') \right] \right]
\end{equation}

类似地，方程 \ref{eq:bellman-equation} 也可以利用迭代的方式计算，并且得到一个最优策略，这种方式称为价值迭代方法。

\subsection{时序差分学习}
在实践中，通常是无法知道条件概率 $p(r, s' | a, s)$，但可以通过智能体与环境交互（采样）获得若干个如序列 \ref{eq:mdp-sequence} 的样本。假设 $V$ 是 $v_\pi$ 的估计，函数$Q(S_t, A_t)$是 $q_\pi$ 的估计。我们可以通过利用采样得到的序列，使用如下算法迭代更新 $Q$ 函数
\begin{equation}
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right]
\end{equation}
这样的算法称为 Sarsa 算法\cite{suttonReinforcementLearningIntroduction2018}。Sarsa 算法是依赖于当前智能体与环境交互时候的$Q$函数，这样使得$Q$函数不能离线更新。而一个称为Q学习\cite{watkinsQlearning1992}的经典算法能够解决离线学习的问题，其更新过程如下：
\begin{equation}
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \right]
\end{equation}


\subsection{策略迭代算法}
前面方法都是基于价值函数，事实上我们的目标只是求得一个策略，而有一类方法直接在智能体调整策略函数$\pi(a|s)$。假设策略依赖于参数$\theta$， $\pi_\theta = \pi(a|s,\theta)$，然后将性能指标定义为：
\begin{equation}
    J(\theta) = \mathbb{E}_\pi \left[ G_t \cdot  \ln(\pi(A_t|S_t, \theta))  \right]
\end{equation}
然后通过梯度上升的方法，最大化 $J(\theta)$。这种方法叫 REINFORCE 算法。而优势行动者-评论者（A2C）算法 \cite{mnihAsynchronousMethodsDeep2016} 则是综合了前面的价值迭代计算与 REINFORCE 算法，利用估计的价值函数和采样的 $R_t$ 估计 $G_t$ 的期望。

\section{深度强化学习}
    近年来，深度学习随着现代计算机计算能力的提升而发挥出惊人的潜能。在强化学习结合深度学习通常被称为深度强化学习。深度强化学习最简单的例子是，在状态空间庞大的时候，利用人工神经网络拟合动作-价值函数$Q$和策略函数$\pi$。


