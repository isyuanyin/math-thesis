%%
% 背景知识
%%

\chapter{背景知识}
\label{cha:background-knowledge}


% TODO 引用
强化学习是为了处理现实中的问题而提出的概念。Sutton 在自己的著作 \cite{suttonReinforcementLearningIntroduction2018}中，将强化学习的理论框架抽象成马尔可夫决策过程（Markov Decision Program, MDP），这也是目前机器学习领域的共识 \cite{mnihHumanlevelControlDeep2015} \cite{mnihAsynchronousMethodsDeep2016} \cite{hesselRainbowCombiningImprovements2018}  ，本文也以此为基础介绍相关的概念和理论。这一章将从马尔可夫决策过程出发，解释强化学习的基本概念，以及介绍一些实践中遇到的问题及其经典的解决方法。

\section{马尔可夫决策过程}
\subsection{基本概念}
% 马尔可夫链（Markov Chain, MC），也称马尔可夫过程，是马尔可夫决策过程的基础。本文只考虑离散时间齐次马尔可夫链假设，即时间（随机变量的参数）可以表示为$T = {0, 1, 2, \dots}$，状态空间是离散的，记为 $\mathcal{S}$。

% \begin{definition}[马尔可夫链]
%     随机变量序列 $S_0, S_1, S_2, \dots$，如果满足（$\mathbb{P}$ 表示概率）：
%     \begin{itemize}
%         \item[] $\mathbb{P} [S_{t} = s' |S_0 = s_0, \dots, S_{t-2} = s_{t-2}, S_{t-1} = s] = \mathbb{P} [S_{t} = s' | S_{t-1} = s]$
%     \end{itemize}
%     则称该随机变量序列具有马尔可夫性质，并且称这样的随机变量序列为马尔可夫链。
% \end{definition}

% \begin{definition}[离散时间齐次马尔可夫链]
%     随机变量序列 $S_0, S_1, S_2, \dots$是离散时间齐次马尔可夫链，如果具有马尔可夫性质并且满足：
%     $\mathbb{P} [S_{t+1} = s' | S_t = s] = \mathcal{P}_{ss'}$，其中$\mathcal{P}$ 是确定的矩阵，称为转移矩阵。
% \end{definition}

% 上述定义表明离散时间齐次马尔可夫链在每个时刻的下一个状态只依赖于当前状态，并且与所在时刻无关。

如第一章所言，马尔可夫决策过程是马尔科夫链在每个状态关联一个动作和奖励，构成一个新的序列，如下所示。

\begin{equation}
    S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3,\dots
    \label{eq:mdp-sequence}
\end{equation}

假设动作空间是 $\mathcal{A}$，奖励的集合是 $\mathcal{R} \subset \mathbb{R}$，即$A_t \in \mathcal{A}$，$R_t \in \mathcal{R}$。马尔可夫决策过程中的下一个状态以及获得的奖励只与当前时刻的状态和动作有关。

\begin{definition}[马尔可夫决策过程]
    一个马尔可夫决策过程是如同序列 \ref{eq:mdp-sequence} 的形式，并且对于任意的$s', s \in \mathcal{S}, r \in \mathcal{R} \text{以及} a\in \mathcal{A}$，满足：
    \begin{equation}
        \mathbb{P}[S_t = s', R_t = r| S_{t-1}, A_{t-1} = a] = p(s', r | s, a)
        \label{def:mdp}
    \end{equation}
\end{definition}

从定义可以看出序列 $\{S_t\}$ 仍然是一个马尔可夫链，只是转移概率与此刻的动作动作关联。从函数 $p(s', r|s, a)$ 可以计算出两个关键的信息，一个是状态转移概率，

\begin{equation}
    p(s'|s, a) = \mathcal{P}[S_t = s' | S_{t-1} = s, A_{t-1} = a] = \sum_{r\in \mathcal{R}} p(s', r|s, a)
\end{equation}
和每个状态-动作的期望收益，
\begin{equation}
    r(s' | s, a) = \mathbb{E} [R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r\in \mathcal{R}} \sum_{s' \in \mathcal{S} p(s', r | s, a)}
\end{equation}


\subsection{回报和折扣}
马尔可夫决策过程的目标是使得总的收益最大，是一类统计决策问题。假设策略是 $\pi = \pi(a | s)$，表示在状态 $s$ 的情况采取 $a$ 动作的概率。考虑一次抽样（即智能体与环境交互一个过程），将得到一个像序列 \ref{eq:mdp-sequence} 的结果。我们将一次采样的累积收益称之为此次采样的回报（return），记为$G$， 即
\begin{equation}
    G = \sum_{t \in T} R_t = R_0 + R_1 + R_2 + \dots
\end{equation}
此外，为了后面的叙述方便，我们可以将每个时刻看作开始时间点，然后计算与该时刻关联的回报。
\begin{equation}
    G_t = \sum_{t' \geq t} R_{t'}
\end{equation}

通常，我们无法改变状态转移的随机性，所以用回报的期望作为优化的目标是合理的。即
\begin{equation}
    \mathbb{E} [G] = \sum_{t \in T} R_t
\end{equation}

\subsection{优化目标 和 Bellman方程}
% 优化问题是
% 从以上可以



\section{实践中的强化学习}
\subsection{动态规划}
% 在时间中全部

\subsection{蒙特卡洛方法}


\subsection{时序差分学习}

\subsection{逼近方法}
    % 如果 $Q$ 函数

\section{深度强化学习}
    % 近年来，深度学习随着现代计算机计算能力的提升而发挥出惊人的潜能。神经网络作为一个逼近器



