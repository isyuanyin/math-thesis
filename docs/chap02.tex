%%
% 背景知识
%%

\chapter{背景知识}
\label{cha:background-knowledge}


% TODO 引用
Sutton在强化学习导论 \cite{suttonReinforcementLearningIntroduction2018}一书中，将强化学习的理论框架抽象成马尔可夫决策过程（Markov Decision Program, MDP），这也是目前机器学习领域的共识 \cite{mnihHumanlevelControlDeep2015} \cite{mnihAsynchronousMethodsDeep2016} \cite{hesselRainbowCombiningImprovements2018}  ，本文也以此为基础介绍相关的概念和理论。这一章将从马尔可夫决策过程出发，解释强化学习的基本概念，以及介绍一些实践中遇到的问题及其经典的解决方法。

\section{马尔可夫决策过程}
\subsection{基本概念}
马尔可夫链（Markov Chain, MC），也称马尔可夫过程，是马尔可夫决策过程的基础，在本文只考虑离散时间齐次马尔可夫链假设，即时间（随机变量的参数）可以表示为$T = {0, 1, 2, \dots}$，状态空间是离散的，记为 $\mathcal{S}$。

\begin{definition}[马尔可夫链]
    随机变量序列 $S_0, S_1, S_2, \dots$，如果满足（$\mathbb{P}$ 表示概率）：
    \begin{itemize}
        \item[] $\mathbb{P} [S_{t} = s' |S_0 = s_0, \dots, S_{t-2} = s_{t-2}, S_{t-1} = s] = \mathbb{P} [S_{t} = s' | S_{t-1} = s]$
    \end{itemize}
    则称该随机变量序列具有马尔可夫性质，并且称这样的随机变量序列为马尔可夫链。
\end{definition}

\begin{definition}[离散时间齐次马尔可夫链]
    随机变量序列 $S_0, S_1, S_2, \dots$是离散时间齐次马尔可夫链，如果具有马尔可夫性质并且满足：
    $\mathbb{P} [S_{t+1} = s' | S_t = s] = \mathcal{P}_{ss'}$，其中$\mathcal{P}$ 是确定的矩阵，称为转移矩阵。
\end{definition}

上述定义表明离散时间齐次马尔可夫链在每个时刻的下一个状态只依赖于当前状态，并且与所在时刻无关。马尔可夫决策过程是马尔科夫链在每个状态关联一个动作和奖励，构成一个新的序列，如下所示。

\begin{equation}
    S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3,\dots
    \label{eq:mdp-sequence}
\end{equation}

假设动作空间是 $\mathcal{A}$，奖励的集合是 $\mathcal{R} \subset \mathbb{R}$，即$A_t \in \mathcal{A}$，$R_t \in \mathcal{R}$。马尔可夫决策过程中的下一个状态以及获得的奖励只与当前时刻的状态和动作有关。

\begin{definition}[马尔可夫决策过程]
    一个马尔可夫决策过程是如同序列 \ref{eq:mdp-sequence} 的形式，并且对于任意的$s', s \in \mathcal{S}, r \in \mathcal{R} \text{以及} a\in \mathcal{A}$，满足：
    \begin{equation}
        \mathbb{P}[S_t = s', R_t = r| S_{t-1}, A_{t-1} = a] = p(s', r | s, a)
        \label{def:mdp}
    \end{equation}
\end{definition}

从定义可以看出，马尔可夫决策过程由一个三元组 $(\mathcal{S}, \mathcal{A}, \mathcal{R})$ 决定，$S_t$ 和并且 $S_0, S_1, S_2, \dots$ 仍然是一个马尔可夫链，只是转移概率与此刻的动作动作关联。从函数 $p(s', r|s, a)$ 可以计算出两个关键的信息，一个是状态转移概率，

\begin{equation}
    p(s'|s, a) = \mathcal{P}[S_t = s' | S_{t-1} = s, A_{t-1} = a] = \sum_{r\in \mathcal{R}} p(s', r|s, a)
\end{equation}
和每个状态-动作的期望收益，
\begin{equation}
    r(s' | s, a) = \mathcal{E} [R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r\in \mathcal{R}} \sum_{s' \in \mathcal{S} p(s', r | s, a)}
\end{equation}


\subsection{汇报和目标}



\subsection{Bellman方程}




\section{实践中的强化学习}


\subsection{动态规划}


\subsection{蒙特卡洛方法}


\subsection{时序差分学习}

\subsection{逼近方法}

\section{深度强化学习}

\subsection{深度学习的逼近方法案例}

\subsection{分类}


